# Directory Structure (Depth 3)
.
├── api
│   ├── config
│   │   ├── logging.yaml
│   │   ├── __pycache__
│   │   └── settings.py
│   ├── connection_manager.py
│   ├── file_handling
│   │   ├── final_output
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   └── temp_input
│   ├── __init__.py
│   ├── logs
│   │   ├── final_logs
│   │   ├── logs
│   │   ├── pipeline.log
│   │   └── run_logs
│   ├── main.py
│   ├── process_video.py
│   ├── __pycache__
│   │   ├── connection_manager.cpython-37.pyc
│   │   ├── exceptions.cpython-37.pyc
│   │   ├── __init__.cpython-37.pyc
│   │   ├── main.cpython-37.pyc
│   │   └── process_video.cpython-37.pyc
│   ├── routes
│   │   ├── file_routes.py
│   │   ├── __init__.py
│   │   ├── processing_routes.py
│   │   ├── __pycache__
│   │   └── ws_routes.py
│   ├── tests
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   ├── test_data
│   │   ├── test_process_video.py
│   │   └── test_utils
│   ├── types
│   │   ├── props.py
│   │   └── __pycache__
│   └── utils
│       ├── analysis_utils.py
│       ├── api_utils.py
│       ├── ffmpeg_utils.py
│       ├── file_utils.py
│       ├── __init__.py
│       ├── log_utils.py
│       ├── __pycache__
│       ├── syncnet_utils.py
│       └── ws_logging_handler.py
├── cleanall.sh
├── cleanup.sh
├── device_config.py
├── download_model.sh
├── __init__ .py
├── output.txt
├── __pycache__
│   └── device_config.cpython-37.pyc
├── README.md
├── requirements.txt
└── syncnet_python
    ├── data
    │   └── work
    ├── demo_feature.py
    ├── demo_syncnet.py
    ├── detectors
    │   ├── __init__.py
    │   ├── __pycache__
    │   ├── README.md
    │   └── s3fd
    ├── img
    │   ├── ex1.jpg
    │   └── ex2.jpg
    ├── __init__.py
    ├── LICENSE.md
    ├── __pycache__
    │   ├── __init__.cpython-37.pyc
    │   ├── run_pipeline.cpython-37.pyc
    │   ├── run_syncnet.cpython-37.pyc
    │   ├── SyncNetInstance.cpython-37.pyc
    │   ├── SyncNetModel.cpython-37.pyc
    │   ├── transformer_syncnet_instance.cpython-37.pyc
    │   └── TransformerSyncNetModel.cpython-37.pyc
    ├── README.md
    ├── requirements.txt
    ├── run_pipeline.py
    ├── run_syncnet.py
    ├── run_visualise.py
    ├── SyncNetInstance.py
    └── SyncNetModel.py

31 directories, 59 files

# Environment Variables
# logging directories
LOGS_BASE=api/logs
LOGS_DIR=api/logs/logs
FINAL_LOGS_DIR=api/logs/final_logs
RUN_LOGS_DIR=api/logs/run_logs
LOG_CONFIG_PATH=api/config/logging.yaml

# processing directories
FILE_HANDLING_DIR=api/file_handling
TEMP_PROCESSING_DIR=api/file_handling/temp_input
FINAL_OUTPUT_DIR=api/file_handling/final_output
DATA_WORK_PYAVI_DIR=syncnet_python/data/work/pyavi
DATA_WORK_DIR=syncnet_python/data/work
DATA_DIR=syncnet_python/data

# processing constants
DEFAULT_MAX_ITERATIONS=1000

# test data directory
TEST_DATA_DIR=api/tests/test_data


# allowed cors origins
ALLOWED_LOCAL_1=http://localhost:3000
ALLOWED_LOCAL_2=http://127.0.0.1:3000




# SyncNet Requirements
aiofiles==23.2.1
aiohttp==3.8.6
aiosignal==1.3.1
annotated-types==0.5.0
anyio==3.7.1
async-timeout==4.0.3
asynctest==0.13.0
attrs==24.2.0
audioread==3.0.1
cached-property==1.5.2
certifi==2024.8.30
cffi==1.15.1
charset-normalizer==3.4.0
click==8.1.7
cycler==0.11.0
decorator==5.1.1
distro==1.9.0
dotenv==0.9.9
exceptiongroup==1.2.2
fastapi==0.103.2
ffmpeg-python==0.2.0
frozenlist==1.3.3
future==1.0.0
h11==0.14.0
httpcore==0.17.3
httpx==0.24.1
idna==3.10
imageio==2.31.2
imageio-ffmpeg==0.5.1
importlib-metadata==6.7.0
importlib-resources==5.12.0
joblib==1.3.2
kiwisolver==1.4.5
librosa==0.7.2
llvmlite==0.39.1
matplotlib==3.3.4
multidict==6.0.5
numba==0.56.4
numpy==1.18.1
opencv-contrib-python==4.10.0.84
opencv-python==4.10.0.84
Pillow==9.5.0
platformdirs==4.0.0
proglog==0.1.10
pycparser==2.21
pydantic==1.10.7
pyparsing==3.1.4
python-dateutil==2.9.0.post0
python-decouple==3.8
python-dotenv==0.21.1
python-magic==0.4.27
python-multipart==0.0.8
python-speech-features==0.6
PyYAML==6.0.1
requests==2.31.0
resampy==0.4.3
scenedetect==0.6.4
scikit-learn==1.0.2
scipy==1.2.1
six==1.16.0
sniffio==1.3.1
soundfile==0.12.1
starlette==0.27.0
threadpoolctl==3.1.0
torch==1.4.0
torchvision==0.5.0
tqdm==4.66.6
typing_extensions==4.7.1
urllib3==2.0.7
uvicorn==0.22.0
watchdog==3.0.0
websockets==11.0.3
yarl==1.9.4
zipp==3.15.0

# Video Processing Script
"""
Module: process_video
Description:
    This module processes a video file by preparing it, synchronizing its audio and video streams using SyncNet,
    and verifying the final synchronization. It leverages asynchronous programming with asyncio to ensure that
    I/O-bound operations (such as reading/writing files and invoking external processes) do not block the event loop.
    Blocking operations (e.g., file operations) are delegated to a thread pool to maintain responsiveness.

Overview:
    The main function, process_video, orchestrates the following workflow:
      1. **Video Preparation:** 
         - Converts the input video to AVI format if required.
         - Retrieves video and audio properties using asynchronous FFmpeg utilities.
         - Copies/moves files using thread pool execution to avoid blocking.
      2. **Synchronization:**
         - Invokes the SyncNet pipeline and model asynchronously to determine audio-video offset.
         - Performs iterative synchronization adjustments if the video is out-of-sync.
      3. **Verification:**
         - Verifies the final synchronization by running a verification pipeline.
      4. **Broadcasting Updates:**
         - Sends status messages via WebSocket to inform clients of progress.

Concurrency and Threading:
    - **Asynchronous Operations:** 
        All major processing steps are implemented as async functions. This allows the use of non-blocking I/O
        (e.g., reading files with aiofiles, running FFmpeg commands with asyncio.create_subprocess_exec).
    - **Thread Pool Execution:**
        Blocking file system operations (such as copying, moving, or deleting files) are executed using:
            - `asyncio.get_running_loop().run_in_executor()`
            - FastAPI's `run_in_threadpool`
        This integration of threading prevents these operations from stalling the asynchronous event loop.
    - **Task Management and Cancellation:**
        The code sets up proper cancellation points and error propagation to handle scenarios where the
        processing pipeline needs to be cancelled or encounters errors.

Function:
    async def process_video(input_file: str, original_filename: str) -> Dict[str, Union[str, bool]]:
        Processes the input video file by preparing it, synchronizing its audio and video, and verifying the
        final synchronization status. It returns a dictionary with either a success message and the final output
        path or error details in case of failure.

    Args:
        input_file (str): The file system path to the uploaded video file.
        original_filename (str): The original name of the uploaded video file.

    Returns:
        Dict[str, Union[str, bool]]:
            - On success:
                {
                    "status": "success",
                    "final_output": "<path to the processed file>",
                    "message": "Video processed successfully."
                }
            - If the video is already synchronized:
                {
                    "already_in_sync": True,
                    "message": "Your clip is already in sync."
                }
            - On error (e.g., missing audio stream, video stream, or fps):
                {
                    "error": True,
                    "message": "<error details>"
                }

    Raises:
        RuntimeError: Propagates any error encountered during video processing.

Usage Example:
    async def main():
        result = await process_video("path/to/video.mp4", "video.mp4")
        if result.get("status") == "success":
            print("Video processed successfully:", result["final_output"])
        else:
            print("Error processing video:", result.get("message"))
    
To run the asynchronous main function, use:
    import asyncio
    asyncio.run(main())
"""

import logging
from typing import Dict, Union
from api.utils.api_utils import ApiUtils
from api.utils.syncnet_utils import SyncNetUtils
from api.types.props import SyncError, ProcessSuccess, ProcessError


logger: logging.Logger = logging.getLogger('process_video')

async def process_video(input_file: str, original_filename: str) -> Union[ProcessSuccess, ProcessError]:
    """
    Processes a video file by preparing it, synchronizing its audio and video using SyncNet, and verifying 
    the synchronization.

    The function implements an asynchronous pipeline that:
      1. Prepares the video by copying/moving the file to the appropriate directory, extracting video 
         and audio properties, and converting it to AVI format if necessary. Blocking file operations are 
         executed via a thread pool.
      2. Synchronizes the video using the SyncNet pipeline:
         - Invokes asynchronous methods to run the SyncNet model and pipeline.
         - Iteratively adjusts the video based on the computed audio-video offset.
      3. Verifies the final synchronization by running an asynchronous verification process.
      4. Sends WebSocket messages to broadcast status updates to the client throughout the process.

    Concurrency and Threading:
      - **Asynchronous Execution:** 
            Utilizes async/await syntax and asyncio's subprocess management to run external commands (e.g., FFmpeg,
            SyncNet) without blocking the event loop.
      - **Thread Pool Usage:** 
            File I/O operations that are blocking are offloaded to a thread pool using methods such as 
            `run_in_threadpool` to ensure the asynchronous workflow remains responsive.
      - **Cooperative Cancellation:** 
            The function provides cancellation points where asynchronous operations can be safely cancelled,
            ensuring that resources are properly cleaned up on exit.
      
    Args:
        input_file (str): Path to the uploaded video file.
        original_filename (str): Original filename of the uploaded video.

    Returns:
        Dict[str, Union[str, bool]]:
            - If processing is successful:
                {
                    "status": "success",
                    "final_output": "<final output file path>",
                    "message": "Video processed successfully."
                }
            - If the video is already synchronized:
                {
                    "already_in_sync": True,
                    "message": "Your clip is already in sync."
                }
            - In case of an error (e.g., missing audio/video streams, fps retrieval issues):
                {
                    "error": True,
                    "message": "<error details>"
                }

    Raises:
        RuntimeError: Propagates any error encountered during processing.
    """
    try:
        avi_file, vid_props, audio_props, fps, destination_path, reference_number = await SyncNetUtils.prepare_video(
            input_file, original_filename
        )
        result_tuple = await SyncNetUtils.synchronize_video(
            avi_file, input_file, original_filename, vid_props, audio_props, fps, destination_path, reference_number
        )
        if isinstance(result_tuple, SyncError):
            return ProcessError(
                error=True,
                message=result_tuple.message,
                no_audio=False,
                no_video=False,
                no_fps=False
            )
        
        final_output, already_in_sync = result_tuple
        if not already_in_sync:
            ref_str: str = f"{reference_number:05d}"
            await SyncNetUtils.verify_synchronization(final_output, ref_str, fps)
            ApiUtils.send_websocket_message("Click the orange circle tick below to get your file! Thanks")
            return ProcessSuccess(
                status="success",
                final_output=final_output,
                message="Video processed successfully."
            )
        else:
            ApiUtils.send_websocket_message("Your clip was already in sync. No changes were made.")
            return ProcessSuccess(
                status="already_in_sync",
                final_output="",
                message="Your clip is already in sync."
            )
    except Exception as e:
        error_msg: str = f"An error occurred during video processing: {e}"
        logger.error(error_msg)
        ApiUtils.send_websocket_message(error_msg)
        err_text: str = str(e)
        if "No audio stream" in err_text:
            return ProcessError(
                error=True,
                no_audio=True,
                message="The video you uploaded has no audio stream inside it"
            )
        elif "Couldn't find any video stream" in err_text:
            return ProcessError(
                error=True,
                no_video=True,
                message="Couldn't see any video stream in the file"
            )
        elif "retrieve fps" in err_text:
            return ProcessError(
                error=True,
                no_fps=True,
                message="Could not retrieve fps"
            )
        else:
            return ProcessError(
                error=True,
                message=err_text
            )

# FastAPI Main App
"""
Main FastAPI application.
"""
import os
import logging
from typing import List
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from api.config.settings import ALLOWED_LOCAL_1, ALLOWED_LOCAL_2
from api.routes.processing_routes import router as processing_router
from api.routes.file_routes import router as file_router
from api.routes.ws_routes import router as ws_router  

logger: logging.Logger = logging.getLogger("uvicorn.info")

app: FastAPI = FastAPI()

origins: List[str] = [ALLOWED_LOCAL_1, ALLOWED_LOCAL_2]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins, 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(processing_router)
app.include_router(file_router)
app.include_router(ws_router)

@app.get("/", tags=["root"])
def read_root() -> dict:
    return {"message": "welcome to sync-api"}

# Props file
from pydantic import BaseModel
from typing import Union, Dict

JSONType = Union[str, int, float, bool, None, dict, list]


class LogConfig(BaseModel):
    __root__: Dict[str, JSONType]

LogConfig.update_forward_refs()

class VideoProps(BaseModel):
    codec_name: str
    avg_frame_rate: str
    fps: float

class AudioProps(BaseModel):
    sample_rate: Union[str, None] = None
    channels: Union[int, None] = None
    codec_name: Union[str, None] = None

class SyncError(BaseModel):
    error: bool
    message: str
    final_offset: int

class ProcessSuccess(BaseModel):
    status: str
    final_output: str
    message: str

class ProcessError(BaseModel):
    error: bool
    no_audio: Union[bool, None] = None
    no_video: Union[bool, None] = None
    no_fps: Union[bool, None] = None
    message: str

class SyncAnalysisResult(BaseModel):
    best_offset_ms: int
    total_confidence: float
    confidence_mapping: Dict[int, float]

# COnnectionManager p
from fastapi import WebSocket
from typing import List
import asyncio

active_connections: List[WebSocket] = []

async def connect(websocket: WebSocket) -> None:
    """
    Accepts a new WebSocket connection and adds it to the active connections list.
    
    Args:
        websocket (WebSocket): The incoming WebSocket connection.
    """
    await websocket.accept()
    active_connections.append(websocket)

async def broadcast(message: str) -> None:
    """
    Sends a text message to all active WebSocket connections.
    
    Args:
        message (str): The message to broadcast.
    """
    if not active_connections:
        return
    await asyncio.gather(
        *[conn.send_text(message) for conn in active_connections],
        return_exceptions=True
    )
def disconnect(websocket: WebSocket) -> None:
    """
    Removes a WebSocket connection from the active connections list.
    
    Args:
        websocket (WebSocket): The WebSocket connection to remove.
    """
    if websocket in active_connections:
        active_connections.remove(websocket)

# Logging Configuration
version: 1
disable_existing_loggers: False

formatters:
  standard:
    format: '[%(asctime)s] [%(levelname)s] %(name)s.%(funcName)s: %(message)s'
  console_format:
    format: '[%(asctime)s] %(funcName)s: %(message)s'

handlers:
  console:
    class: logging.StreamHandler
    level: DEBUG
    formatter: console_format
    stream: ext://sys.stdout

  app_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/app.log
    maxBytes: 1048576 
    backupCount: 5
    encoding: utf8

  process_video_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/process_video.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  ffmpeg_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/ffmpeg.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  pipeline_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/pipeline.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  analysis_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/analysis.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  file_utils_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/file_utils.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  uvicorn_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: INFO
    formatter: standard
    filename: api/logs/logs/uvicorn.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  file_routes_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/file_routes.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8

  processing_routes_file_handler:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: standard
    filename: api/logs/logs/processing_routes.log
    maxBytes: 1048576
    backupCount: 5
    encoding: utf8


loggers:
  process_video:
    handlers: [process_video_file_handler]
    level: DEBUG
    propagate: false

  multipart:
    handlers: [app_file_handler]  
    level: WARNING
    propagate: false

  ffmpeg_logger:
    handlers: [ffmpeg_file_handler]
    level: DEBUG
    propagate: false

  pipeline_logger:
    handlers: [pipeline_file_handler, app_file_handler]
    level: DEBUG
    propagate: false

  analysis_logger:
    handlers: [analysis_file_handler, console, app_file_handler]
    level: DEBUG
    propagate: false

  file_utils_logger:
    handlers: [file_utils_file_handler, console, app_file_handler]
    level: DEBUG
    propagate: false

  api_logger:
    handlers: [console, app_file_handler]
    level: DEBUG
    propagate: false

  fastapi:
    handlers: [console, app_file_handler]
    level: DEBUG
    propagate: false

  uvicorn:
    handlers: [uvicorn_file_handler, console, app_file_handler]
    level: INFO
    propagate: false

  uvicorn.error:
    handlers: [uvicorn_file_handler, console, app_file_handler]
    level: ERROR
    propagate: false

  uvicorn.access:
    handlers: [uvicorn_file_handler, console, app_file_handler]
    level: INFO
    propagate: false

  file_routes:
    handlers: [file_routes_file_handler, console, app_file_handler]
    level: DEBUG
    propagate: false

  processing_routes:
    handlers: [processing_routes_file_handler, console, app_file_handler]
    level: DEBUG
    propagate: false

  api_utils_logger:
    handlers: [app_file_handler, console]
    level: DEBUG
    propagate: false

  log_utils_logger:
    handlers: [app_file_handler, console]
    level: DEBUG
    propagate: false

  syncnet_utils_logger:
    handlers: [pipeline_file_handler, app_file_handler]
    level: DEBUG
    propagate: false

  asyncio:
    level: WARNING
    propagate: false


root:
  handlers: [app_file_handler, console]
  level: DEBUG

# Settings Configuration
import os
from dotenv import load_dotenv

# determine the project root directory 
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))

# load the .env file from the project root
dotenv_path = os.path.join(BASE_DIR, ".env")
load_dotenv(dotenv_path=dotenv_path, override=True)


# logging directories 
LOGS_BASE = os.path.join(BASE_DIR, os.getenv("LOGS_BASE", "api/logs"))
LOGS_DIR = os.path.join(BASE_DIR, os.getenv("LOGS_DIR", "api/logs/logs"))
FINAL_LOGS_DIR = os.path.join(BASE_DIR, os.getenv("FINAL_LOGS_DIR", "api/logs/final_logs"))
RUN_LOGS_DIR = os.path.join(BASE_DIR, os.getenv("RUN_LOGS_DIR", "api/logs/run_logs"))
LOG_CONFIG_PATH = os.path.join(BASE_DIR, os.getenv("LOG_CONFIG_PATH", "api/config/logging.yaml"))

# processing directories (
FILE_HANDLING_DIR = os.path.join(BASE_DIR, os.getenv("FILE_HANDLING_DIR", "api/file_handling"))
TEMP_PROCESSING_DIR = os.path.join(BASE_DIR, os.getenv("TEMP_PROCESSING_DIR", "api/file_handling/temp_input"))
FINAL_OUTPUT_DIR = os.path.join(BASE_DIR, os.getenv("FINAL_OUTPUT_DIR", "api/file_handling/final_output"))
DATA_WORK_PYAVI_DIR = os.path.join(BASE_DIR, os.getenv("DATA_WORK_PYAVI_DIR", "syncnet_python/data/work/pyavi"))
DATA_WORK_DIR = os.path.join(BASE_DIR, os.getenv("DATA_WORK_DIR", "syncnet_python/data/work"))
DATA_DIR = os.path.join(BASE_DIR, os.getenv("DATA_DIR", "syncnet_python/data"))


# processing constants
DEFAULT_MAX_ITERATIONS = int(os.getenv("DEFAULT_MAX_ITERATIONS", 30))

# test data directory
TEST_DATA_DIR = os.path.join(BASE_DIR, os.getenv("TEST_DATA_DIR", "api/tests/test_data"))

# allowed cors origins
ALLOWED_LOCAL_1 = os.getenv("ALLOWED_LOCAL_1", "http://localhost:3000")
ALLOWED_LOCAL_2 = os.getenv("ALLOWED_LOCAL_2", "http://127.0.0.1:3000")
ALLOWED_ORIGINS = [ALLOWED_LOCAL_1, ALLOWED_LOCAL_2]


# Route Files
import os
import aiofiles
from fastapi import APIRouter, HTTPException
from fastapi.responses import FileResponse
import logging
from api.config.settings import FINAL_OUTPUT_DIR

router = APIRouter()
logger = logging.getLogger("file_routes")



@router.get("/download/{filename}")
async def download_file(filename: str) -> FileResponse:
    """Handles file download requests by asynchronously verifying file existence 
    and returning a FileResponse if the file is accessible.

    Args:
        filename (str): The name of the file to be downloaded.

    Returns:
        FileResponse: A response object that streams the requested file.

    Raises:
        HTTPException: 404 if the file does not exist.
        HTTPException: 500 if an error occurs while accessing the file.
    """
    logger.debug(f"[ENTER] download_file - Requested download for filename='{filename}'")
    file_path: str = os.path.join(FINAL_OUTPUT_DIR, filename)
    logger.debug(f"[download_file] Constructed file_path='{file_path}'")

    if not os.path.isfile(file_path):
        logger.error(f"[download_file] File not found -> {file_path}")
        raise HTTPException(status_code=404, detail="file not found.")

    logger.info(f"[download_file] File found. Preparing to return FileResponse -> {file_path}")

    try:
        async with aiofiles.open(file_path, "rb") as f:
            await f.read(10)
    except Exception as e:
        logger.error(f"[download_file] Error accessing file -> {file_path}, Error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error.")

    logger.debug(f"[EXIT] download_file -> returning FileResponse for '{filename}'")
    return FileResponse(file_path, filename=filename)
from fastapi import APIRouter, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from api.utils.api_utils import ApiUtils
from api.process_video import process_video
from api.types.props import ProcessSuccess, ProcessError
import os
import logging
from fastapi.concurrency import run_in_threadpool

logger: logging.Logger = logging.getLogger("processing_routes")
router: APIRouter = APIRouter()

@router.post("/process")
async def process_video_endpoint(file: UploadFile = File(...)) -> JSONResponse:
    """
    Processes an uploaded video file by invoking the video synchronization pipeline and returns a JSON response
    with either a download URL for the processed file or an appropriate status message.

    This endpoint performs the following steps:
      1. Saves the uploaded file to a temporary directory using ApiUtils.save_temp_file.
      2. Calls the asynchronous process_video function to process the video:
         - If the video is successfully processed and modified, verifies the existence of the final output file,
           extracts its filename, and returns a JSON response containing the filename and a download URL.
         - If the video is already synchronized, returns a JSON response with an appropriate message.
         - If an error occurs during processing, returns a JSON response with error details.
      3. In all cases, ensures that the temporary input file is deleted after processing, using a thread pool
         to handle the blocking I/O operation.

    Args:
        file (UploadFile): The uploaded video file received from the client.

    Returns:
        JSONResponse: A JSON response containing:
            - For successful processing:
                  {
                      "filename": "<final_output_filename>",
                      "url": "/download/<final_output_filename>"
                  }
            - For a video that is already in sync:
                  {
                      "message": "Your clip is already in sync."
                  }
            - For an error during processing:
                  A JSON object with error details as defined by the ProcessError Pydantic model.

    Raises:
        HTTPException: If the final output file is missing after processing, or if any unexpected error occurs
                       during the processing pipeline. The HTTP status code 500 is returned in such cases.

    Examples:
        To use this endpoint, a client would typically perform a POST request to /process with the video file:
        
        >>> import requests
        >>> files = {'file': open('video.mp4', 'rb')}
        >>> response = requests.post("http://localhost:8000/process", files=files)
        >>> print(response.json())
    """
    input_file_path: str = await ApiUtils.save_temp_file(file)
    try:
        result = await process_video(input_file_path, file.filename)
        if isinstance(result, ProcessSuccess):
            if result.status == "success" and result.final_output:
                if not os.path.exists(result.final_output):
                    logger.error("Final output file does not exist.")
                    raise HTTPException(status_code=500, detail="processing failed.")
                final_filename: str = os.path.basename(result.final_output)
                return JSONResponse(content={
                    "filename": final_filename,
                    "url": f"/download/{final_filename}"
                })
            elif result.status == "already_in_sync":
                return JSONResponse(content={
                    "message": result.message
                })
        elif isinstance(result, ProcessError):
            return JSONResponse(content=result.dict())
        
        logger.error("Unexpected result type from process_video.")
        raise HTTPException(status_code=500, detail="processing failed.")
        
    except Exception as e:
        logger.error(f"An error occurred: {e}")
        raise HTTPException(status_code=500, detail="processing failed.") from e
    finally:
        if os.path.exists(input_file_path):
            await run_in_threadpool(os.remove, input_file_path)
"""
WebSocket routes.
"""
from fastapi import APIRouter, WebSocket, WebSocketDisconnect
from api.connection_manager import connect, disconnect

router = APIRouter()

@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket) -> None:
    """
    WebSocket endpoint that echoes received messages.
    
    Args:
        websocket (WebSocket): The incoming WebSocket connection.
    """
    print("[ENTER] websocket_endpoint called")
    try:
        await connect(websocket)
        print("[websocket_endpoint] WebSocket connected successfully.")
    except Exception as e:
        print(f"[websocket_endpoint] Error accepting connection: {e}")
        return

    try:
        while True:
            data: str = await websocket.receive_text()
            print(f"[websocket_endpoint] Received text from client: {data}")
            await websocket.send_text(f"Echo: {data}")
    except WebSocketDisconnect:
       print("[websocket_endpoint] Client disconnected.")
       disconnect(websocket)
    print("[EXIT] websocket_endpoint completed")

# Utility Scripts
"""
Async log analysis utilities with hybrid async/threaded processing for SyncNet log analysis.
"""

import re
import logging
import asyncio
from typing import List, Tuple, Dict, Union
from api.utils.file_utils import FileUtils
from api.types.props import SyncAnalysisResult
from collections import defaultdict
from api.utils.api_utils import ApiUtils

logger: logging.Logger = logging.getLogger('analysis_logger')

class AnalysisUtils:
    """Provides static methods for asynchronous log analysis using a hybrid async/threaded approach.
    
    This class combines asynchronous I/O operations with threaded CPU-bound processing to efficiently
    analyze SyncNet log files while maintaining responsive performance.
    """

    @staticmethod
    async def analyze_syncnet_log(log_filename: str, fps: Union[int, float]) -> SyncAnalysisResult:       
        """Asynchronous pipeline for analyzing SyncNet log files.

        Performs the complete analysis workflow:
        1. Read log file asynchronously
        2. Extract offset/confidence pairs
        3. Aggregate confidence values
        4. Determine optimal offset
        5. Convert frames to milliseconds

        Args:
            log_filename: Path to the SyncNet log file to analyze
            fps: Video frames per second for milliseconds conversion

        Returns:
            SyncAnalysisResult: Contains the best offset in milliseconds, the total confidence, 
                                and a mapping of offsets to their confidence values.
        
        Raises:
            Exception: Captures and logs any unexpected errors during processing

        Example:
                best_offset = await AnalysisUtils.analyze_syncnet_log('sync.log', 29.97)
                print(f"Optimal sync offset: {best_offset}ms")
        """
        logger.debug(f"Analyzing SyncNet log: {log_filename}")
        try:
            log_content = await FileUtils.read_file(log_filename)
            if not log_content:
                logger.warning(f"Empty log file: {log_filename}")
                return SyncAnalysisResult(best_offset_ms=0, total_confidence=0.0, confidence_mapping={})
            
            pairs = await ApiUtils.run_blocking(AnalysisUtils.extract_offset_confidence_pairs, log_content)
            if not pairs:
                logger.warning("No offset/confidence pairs found")
                return SyncAnalysisResult(best_offset_ms=0, total_confidence=0.0, confidence_mapping={})
            
            confidence_map = await ApiUtils.run_blocking(AnalysisUtils.aggregate_confidence, pairs)
            if not confidence_map:
                return SyncAnalysisResult(best_offset_ms=0, total_confidence=0.0, confidence_mapping={})
            
            best_offset = max(confidence_map, key=confidence_map.get)
            total_confidence = sum(confidence_map.values())
            best_offset_ms = AnalysisUtils.convert_frames_to_ms(best_offset, fps)
            
            return SyncAnalysisResult(
                best_offset_ms=best_offset_ms,
                total_confidence=total_confidence,
                confidence_mapping=confidence_map
            )
        except Exception as e:
            logger.error(f"Analysis failed: {str(e)}")
            return SyncAnalysisResult(best_offset_ms=0, total_confidence=0.0, confidence_mapping={})


    @staticmethod
    def extract_offset_confidence_pairs(log_text: str) -> List[Tuple[int, float]]:
        """Extracts offset and confidence pairs from SyncNet log content.

        Uses regular expressions to find patterns matching:
        'AV offset: [number] Confidence: [decimal]'

        Args:
            log_text: Raw text content from SyncNet log file

        Returns:
            List[Tuple[int, float]]: List of tuples containing (offset, confidence) pairs.
            Returns empty list if no valid matches found.

        Example:
                pairs = AnalysisUtils.extract_offset_confidence_pairs(log_content)
                print(pairs)
                [(-2, 5.43), (3, 7.21), ...]
        """
        pairs = []
        pattern = r'AV offset:\s*(-?\d+).*?Confidence:\s*([\d.]+)'
        matches = re.findall(pattern, log_text, re.DOTALL)
        for offset_str, confidence_str in matches:
            try:
                offset = int(offset_str)
                confidence = float(confidence_str)
                pairs.append((offset, confidence))
            except (ValueError, TypeError):
                logger.debug(f"Skipping invalid pair: {offset_str}, {confidence_str}")
                continue
        return pairs

    @staticmethod
    def aggregate_confidence(pairs: List[Tuple[int, float]]) -> Dict[int, float]:
        """Aggregates confidence values for each unique offset.

        Processes a list of (offset, confidence) pairs to create:
        - Summed confidence values per offset
        - Filters out negative confidence values

        Args:
            pairs: List of (offset, confidence) tuples

        Returns:
            Dict[int, float]: Dictionary mapping offsets to total confidence.
            Returns empty dict if no valid pairs provided.

        Example:
                confidence_map = AnalysisUtils.aggregate_confidence(pairs)
                print(confidence_map)
                {-2: 12.84, 3: 24.56, ...}
        """
        confidence_map = defaultdict(float)
        for offset, confidence in pairs:
            if confidence < 0:
                logger.debug(f"Skipping negative confidence: {confidence}")
                continue
            confidence_map[offset] += confidence
        return confidence_map

    @staticmethod
    def convert_frames_to_ms(frames: int, fps: Union[int, float]) -> int:
        """Converts frame offset to milliseconds using video FPS."""
        try:
            if fps == 0: 
                return 0
            return int((frames * 1000) / fps)
        except Exception as error:
            logger.debug(f"Conversion error: {error} (frames={frames}, fps={fps})")
            return 0
"""
Utility class for handling API operations.
"""

import os
import shutil
import uuid
import logging
import asyncio
from fastapi import UploadFile
from api.connection_manager import broadcast
from api.utils.log_utils import LogUtils
from api.config.settings import TEMP_PROCESSING_DIR

LogUtils.configure_logging()
logger: logging.Logger = logging.getLogger("api_utils_logger")
import aiofiles

class ApiUtils:

    @staticmethod
    async def run_blocking(func, *args, **kwargs):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, func, *args, **kwargs)

    @staticmethod
    async def save_temp_file(uploaded_file: UploadFile) -> str:
        """
        Saves an uploaded file to a temporary directory with a unique name.
        
        Args:
            uploaded_file (UploadFile): The file uploaded by the client.
        
        Returns:
            str: Path to the saved temporary file.
        
        Raises:
            IOError: If the file cannot be saved.
        """
        file_extension = os.path.splitext(uploaded_file.filename)[1]
        unique_filename = os.path.join(TEMP_PROCESSING_DIR, f"{uuid.uuid4()}{file_extension}")
        try:
            async with aiofiles.open(unique_filename, "wb") as temp_file:
                content = await uploaded_file.read()
                await temp_file.write(content)
            return unique_filename
        except Exception as e:
            logger.error(f"Failed to save temp file: {e}")
            raise

    @staticmethod
    def send_websocket_message(message: str) -> None:
        """
        Broadcasts a message to connected WebSocket clients.

        Args:
            message (str): The message to send.
        """
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            loop.create_task(broadcast(message))
        else:
            new_loop = asyncio.new_event_loop()
            try:
                new_loop.run_until_complete(broadcast(message))
            finally:
                new_loop.close()
import os
import json
import logging
import shutil
import asyncio
from typing import Optional, Dict, List, Union
from api.config.settings import FINAL_OUTPUT_DIR
from api.types.props import VideoProps, AudioProps
from api.utils.file_utils import FileUtils
from api.utils.api_utils import ApiUtils

logger: logging.Logger = logging.getLogger("ffmpeg_logger")


class FFmpegUtils:
    """ Utility class for handling various FFmpeg operations asynchronously.
        This class provides methods for re-encoding videos to AVI format, converting AVI
        files back to their original container/codec, shifting audio tracks by a specified
        offset, applying cumulative audio shifts, and extracting audio and video properties
        using ffprobe. All operations are executed using asyncio subprocesses to prevent
        blocking the event loop during CPU-intensive or long-running FFmpeg tasks.
    
    Implements async patterns for:
    - Parallel video encoding/transcoding
    - Non-blocking stream manipulation
    - Async property analysis with FFprobe
    - Resource-safe process cleanup

    Subprocess Handling:
    - All FFmpeg/FFprobe commands use asyncio.subprocess
    - stdin/stderr captured asynchronously
    - Configurable timeouts for long-running encodes
    """

    @staticmethod
    async def reencode_to_avi(input_file: str, output_file: str) -> None:
        """Re-encodes a given input video file to an AVI format with specific codecs.

        Args:
            input_file (str): Path to the source file to be converted.
            output_file (str): Desired path of the converted AVI file.

        Raises:
            RuntimeError: If the ffmpeg command fails with a non-zero exit code.
        """
        logger.debug(f"[ENTER] reencode_to_avi -> input_file='{input_file}', output_file='{output_file}'")
        cmd = [
            "ffmpeg",
            "-y",
            "-i", input_file,
            "-vcodec", "mpeg4",
            "-acodec", "pcm_s16le",
            "-strict", "experimental",
            output_file
        ]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            error_msg = stderr.decode("utf-8", "ignore")
            logger.error(f"[reencode_to_avi] FFmpeg error -> {error_msg}")
            raise RuntimeError(f"Failed to re-encode to AVI: {error_msg}")
        logger.debug("[EXIT] reencode_to_avi")

    @staticmethod
    async def reencode_to_original_format(
        input_avi_file: str,
        output_file: str,
        original_container_ext: str,
        original_video_codec: Optional[str],
        original_audio_codec: Optional[str]
    ) -> None:
        """Re-encodes an AVI file back to its original container/codec.

        Args:
            input_avi_file (str): Path to the intermediate AVI file.
            output_file (str): Desired path of the final restored video.
            original_container_ext (str): File extension of the original container (e.g. '.mp4').
            original_video_codec (Optional[str]): Original video codec if known.
            original_audio_codec (Optional[str]): Original audio codec if known.

        Raises:
            RuntimeError: If the ffmpeg command fails or if re-encoding fails.
        """
        logger.debug(
            f"[ENTER] reencode_to_original_format -> input_avi_file='{input_avi_file}', "
            f"output_file='{output_file}', original_container_ext='{original_container_ext}', "
            f"original_video_codec='{original_video_codec}', original_audio_codec='{original_audio_codec}'"
        )
        vcodec = original_video_codec if original_video_codec else "copy"
        acodec = original_audio_codec if original_audio_codec else "copy"
        cmd = [
            "ffmpeg",
            "-y",
            "-i", input_avi_file,
            "-vcodec", vcodec,
            "-acodec", acodec,
            output_file
        ]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            error_msg = stderr.decode("utf-8", "ignore")
            logger.error(f"[reencode_to_original_format] FFmpeg error -> {error_msg}")
            raise RuntimeError(
                f"Failed to re-encode to original container {original_container_ext}: {error_msg}"
            )
        logger.debug("[EXIT] reencode_to_original_format")

    @staticmethod
    async def shift_audio(input_file: str, output_file: str, offset_ms: int) -> None:
        """Async audio shifting with FFmpeg.
        
        Async Implementation:
        - Full async subprocess lifecycle management
        - Non-blocking audio property analysis
        - Async filter graph construction
        - Thread-safe output file handling

        Flow Control:
        - Creates dedicated event loop for filter operations
        - Uses async file existence checks
        - Implements backpressure through queueingShifts the audio track of a file either forwards or backwards by a given offset.

        Args:
            input_file (str): Path to the source video.
            output_file (str): Desired path of the shifted-output file.
            offset_ms (int): Millisecond offset to apply. Positive for forward, negative for backward.

        Raises:
            RuntimeError: If the ffmpeg operation fails or if the input file is missing.
        """
        logger.debug(
            f"[ENTER] shift_audio -> input_file='{input_file}', output_file='{output_file}', offset_ms={offset_ms}"
        )
        exists = await ApiUtils.run_blocking(os.path.exists, input_file)
        if not exists:
            logger.error(f"[shift_audio] Input file not found -> '{input_file}'")
            return
        audio_props = await FFmpegUtils.get_audio_properties(input_file)
        logger.debug(f"[shift_audio] audio_props -> {audio_props}")
        if audio_props is None:
            logger.error(f"[shift_audio] No audio props found in '{input_file}'")
            return
        sample_rate = int(audio_props.get("sample_rate"))
        channels = int(audio_props.get("channels"))
        codec_name = audio_props.get("codec_name")
        if offset_ms > 0:
            filter_complex = f"adelay={offset_ms}|{offset_ms},apad"
            logger.info(f"[shift_audio] Shifting audio FORWARD by {offset_ms} ms.")
        else:
            shift_abs = abs(offset_ms)
            filter_complex = f"atrim=start={shift_abs / 1000},apad"
            logger.info(f"[shift_audio] Shifting audio BACKWARD by {shift_abs} ms.")
        cmd = [
            "ffmpeg",
            "-y",
            "-i", input_file,
            "-c", "copy",
            "-af", filter_complex,
            "-c:a", codec_name,
            "-ar", str(sample_rate),
            "-ac", str(channels),
            "-threads", "4",
            "-shortest",
            output_file
        ]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            error_msg = stderr.decode("utf-8", "ignore")
            logger.error(f"[shift_audio] FFmpeg error -> {error_msg}")
            raise RuntimeError(f"Error shifting audio for {input_file}: {error_msg}")
        logger.debug("[EXIT] shift_audio")

    @staticmethod
    async def apply_cumulative_shift(input_file: str, final_output: str, total_shift_ms: int) -> None:
        """Copies a file into the final output directory, then applies a global audio shift.

        Args:
            input_file (str): Source file to be shifted.
            final_output (str): Desired path of the final shifted file.
            total_shift_ms (int): Total millisecond offset to shift the audio.

        Raises:
            RuntimeError: If any subprocess errors occur during the shift operation.
        """
        logger.debug(
            f"[ENTER] apply_cumulative_shift -> input_file='{input_file}', final_output='{final_output}', "
            f"total_shift_ms={total_shift_ms}"
        )
        copied_file = os.path.join(FINAL_OUTPUT_DIR, os.path.basename(input_file))
        await FileUtils.copy_file(input_file, copied_file)
        logger.debug(f"[apply_cumulative_shift] Copied input -> '{copied_file}'")
        try:
            await FFmpegUtils.shift_audio(copied_file, final_output, total_shift_ms)
            logger.info(f"[apply_cumulative_shift] Completed shift. final_output='{final_output}'")
        except Exception as e:
            logger.error(f"[apply_cumulative_shift] Exception -> {str(e)}")
            raise RuntimeError(f"Could not apply cumulative shift: {e}")
        finally:
            exists = await ApiUtils.run_blocking(os.path.exists, copied_file)
            if exists:
                await FileUtils.cleanup_file(copied_file)
                logger.debug(f"[apply_cumulative_shift] Removed temp file -> '{copied_file}'")
        logger.debug("[EXIT] apply_cumulative_shift")

    @staticmethod
    async def get_audio_properties(file_path: str) -> Optional[AudioProps]:
        """Retrieves audio properties from the given file using ffprobe.

        Args:
            file_path (str): Path to the input media file.

        Returns:
            Optional[AudioProps]: A dictionary containing audio information
            (sample_rate, channels, codec_name), or None if no audio stream is found.

        Raises:
            RuntimeError: If ffprobe fails to execute.
        """
        logger.debug(f"[ENTER] get_audio_properties -> file_path='{file_path}'")
        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_streams",
            file_path
        ]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            error_msg = stderr.decode("utf-8", "ignore")
            logger.error(f"[get_audio_properties] FFprobe error -> {error_msg}")
            return None
        try:
            metadata = json.loads(stdout.decode("utf-8", "ignore"))
            streams = metadata.get("streams", [])
            for stream in streams:
                if stream.get("codec_type") == "audio":
                    audio_props: AudioProps = {
                        "sample_rate": stream.get("sample_rate"),
                        "channels": stream.get("channels"),
                        "codec_name": stream.get("codec_name")
                    }
                    logger.info(f"[get_audio_properties] Found audio props -> {audio_props}")
                    logger.debug(f"[EXIT] get_audio_properties -> {audio_props}")
                    return audio_props
            logger.error(f"[get_audio_properties] No audio stream found in '{file_path}'")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"[get_audio_properties] JSON parsing error -> {str(e)}")
            return None

    @staticmethod
    async def get_video_properties(file_path: str) -> Optional[VideoProps]:
        """Retrieves video properties from the given file using ffprobe.

        Args:
            file_path (str): Path to the input media file.

        Returns:
            Optional[VideoProps]: A dictionary containing video information
            (codec_name, avg_frame_rate, fps), or None if no video stream is found.

        Raises:
            RuntimeError: If ffprobe fails to execute.
        """
        logger.debug(f"[ENTER] get_video_properties -> file_path='{file_path}'")
        cmd = [
            "ffprobe",
            "-v", "quiet",
            "-print_format", "json",
            "-show_streams",
            file_path
        ]
        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await proc.communicate()
        if proc.returncode != 0:
            error_msg = stderr.decode("utf-8", "ignore")
            logger.error(f"[get_video_properties] ffprobe error -> {error_msg}")
            return None
        try:
            metadata = json.loads(stdout.decode("utf-8", "ignore"))
            streams = metadata.get("streams", [])
            for stream in streams:
                if stream.get("codec_type") == "video":
                    avg_frame_rate = stream.get("avg_frame_rate", "0/0")
                    fps = 0.0
                    try:
                        num, den = avg_frame_rate.split("/")
                        if float(den) != 0:
                            fps = float(num) / float(den)
                    except Exception as e:
                        logger.error(f"[get_video_properties] Error parsing avg_frame_rate='{avg_frame_rate}' -> {str(e)}")
                    video_props: VideoProps = {
                        "codec_name": stream.get("codec_name"),
                        "avg_frame_rate": avg_frame_rate,
                        "fps": fps
                    }
                    logger.info(f"[get_video_properties] Found video props -> {video_props}")
                    logger.debug(f"[EXIT] get_video_properties -> {video_props}")
                    return video_props
            logger.info(f"[get_video_properties] No video stream found in '{file_path}'")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"[get_video_properties] JSON parsing error -> {str(e)}")
            return None
import os
import shutil
import logging
from typing import List, Optional
import aiofiles, asyncio
from asyncio import get_running_loop
from api.utils.api_utils import ApiUtils

logger: logging.Logger = logging.getLogger('file_utils_logger')


class FileUtils:
    @staticmethod
    async def copy_file(source: str, destination: str) -> str:
        """Async copy using threadpool for blocking I/O."""
        logger.debug(f"Copying file: {source} -> {destination}")
        try:
            await ApiUtils.run_blocking(shutil.copy, source, destination)
            logger.info(f"Copied file: {source} -> {destination}")
            return destination
        except Exception as e:
            logger.error(f"Failed to copy file: {e}")
            raise IOError(f"Could not copy file: {e}")

    @staticmethod
    async def move_file(source: str, destination: str) -> str:
        """Async move using threadpool for blocking I/O."""
        logger.debug(f"Moving file: {source} -> {destination}")
        try:
            await ApiUtils.run_blocking(shutil.move, source, destination)
            logger.info(f"Moved file: {source} -> {destination}")
            return destination
        except Exception as e:
            logger.error(f"Failed to move file: {e}")
            raise IOError(f"Could not move file: {e}")

    @staticmethod
    async def read_file(file_path: str) -> str:
        """Async file read using aiofiles."""
        logger.debug(f"Reading file: {file_path}")
        try:
            async with aiofiles.open(file_path, "r") as f:
                content = await f.read()
            logger.debug(f"Successfully read file: {file_path}")
            return content
        except Exception as e:
            logger.error(f"Failed to read file: {e}")
            raise IOError(f"Could not read file: {e}")

    @staticmethod
    async def cleanup_file(file_path: str) -> None:
        """Async file deletion using threadpool for blocking I/O."""
        logger.debug(f"Cleaning up file: {file_path}")
        try:
            await ApiUtils.run_blocking(os.remove, file_path)
            logger.info(f"Removed file: {file_path}")
        except FileNotFoundError:
            logger.warning(f"File not found: {file_path}")
        except Exception as e:
            logger.error(f"Failed to remove file: {e}")
            raise IOError(f"Could not remove file: {e}")

    @staticmethod
    async def get_next_directory_number(data_dir: str) -> str:
        """Async directory number calculation"""
        try:
            items = await ApiUtils.run_blocking(os.listdir, data_dir)
            existing_numbers = [int(item) for item in items if item.isdigit()]
            next_number = max(existing_numbers) + 1 if existing_numbers else 1
            return f"{next_number:05d}"
        except FileNotFoundError:
            await ApiUtils.run_blocking(os.makedirs, data_dir, exist_ok=True)
            return "00001"
import os
import logging
import logging.config
import yaml
from api.config.settings import LOG_CONFIG_PATH
from api.types.props import LogConfig

class LogUtils:
    @staticmethod
    def configure_logging() -> None:
        logger = logging.getLogger("log_utils_logger")
        logger.debug("[ENTER] configure_logging")
        try:
            with open(LOG_CONFIG_PATH, 'r') as file:
                config: LogConfig = yaml.safe_load(file)
        except FileNotFoundError:
            logger.error(f"[configure_logging] Couldn't find logging config -> '{LOG_CONFIG_PATH}'")
            raise
        logging.config.dictConfig(config)
        logger.debug("[EXIT] configure_logging")
"""Module for asynchronous operations related to SyncNet.

This module contains the SyncNetUtils class, which provides asynchronous wrappers
for running the SyncNet pipeline and related FFmpeg operations. It uses asyncio’s
subprocess API to run external commands without blocking the event loop.

Attributes:
    logger (logging.Logger): Logger for the module.
"""

import os, shutil, asyncio, aiofiles
from typing import Tuple, Union, Optional, Dict
import logging

from api.config.settings import (
    DEFAULT_MAX_ITERATIONS,
    TEMP_PROCESSING_DIR,
    FINAL_LOGS_DIR,
    FINAL_OUTPUT_DIR,
    DATA_WORK_PYAVI_DIR,
    DATA_WORK_DIR,
    DATA_DIR
)
from api.utils.api_utils import ApiUtils
from api.utils.file_utils import FileUtils
from api.utils.ffmpeg_utils import FFmpegUtils
from api.utils.analysis_utils import AnalysisUtils
from api.types.props import VideoProps, AudioProps, SyncError

logger: logging.Logger = logging.getLogger('process_video')


class SyncNetUtils:
    """A collection of asynchronous utility methods for running SyncNet and FFmpeg tasks.

    This class encapsulates methods to run the SyncNet model, SyncNet pipeline, and
    associated video/audio processing.

        A collection of asynchronous utility methods for running SyncNet and FFmpeg tasks
    
            Implements async patterns for CPU-intensive media processing using
            Asyncio subprocess management for FFmpeg/SyncNet binaries
            Thread pool execution for blocking I/O operations
            Async context managers for file handling
            Cooperative task cancellation support

        All methods are intended to be non-blocking and event-loop friendly

    Methods:
        run_syncnet(ref_str: str, log_file: Optional[str] = None) -> str:
            Runs the SyncNet model asynchronously and returns the log file path.
        run_pipeline(video_file: str, ref: str) -> None:
            Runs the SyncNet pipeline asynchronously.
        prepare_video(input_file: str, original_filename: str) -> Tuple[str, VideoProps, AudioProps, Union[int, float], str, int]:
            Prepares a video file for synchronization and returns the AVI file path,
            video properties, audio properties, frame rate, destination path, and reference number.
        perform_sync_iterations(corrected_file: str, original_filename: str, fps: Union[int, float], reference_number: int) -> Union[SyncError, Tuple[int, str, int, int]]:
            Performs iterative synchronization using SyncNet and returns either a SyncError
            or a tuple with total shift in ms, corrected file, updated reference number, and iteration count.
        finalize_sync(input_file: str, original_filename: str, total_shift_ms: int, reference_number: int, fps: Union[int, float], destination_path: str, vid_props: VideoProps, audio_props: AudioProps, corrected_file: str) -> Union[str, SyncError]:
            Finalizes the synchronization process and returns either the final output path or a SyncError.
        synchronize_video(avi_file: str, input_file: str, original_filename: str, vid_props: VideoProps, audio_props: AudioProps, fps: Union[int, float], destination_path: str, reference_number: int) -> Union[Tuple[str, bool], SyncError]:
            Orchestrates the entire synchronization process and returns a tuple with the final
            output path and a boolean indicating if the clip was already synchronized, or a SyncError.
        verify_synchronization(final_path: str, ref_str: str, fps: Union[int, float]) -> None:
            Verifies the synchronization of the final output video.
    """

    @staticmethod
    async def run_syncnet(ref_str: str, log_file: Optional[str] = None) -> str:
        logger.debug(f"[run_syncnet][ENTER] ref_str='{ref_str}', log_file='{ref_str}'")
        if log_file is None:
            log_file = os.path.join(FINAL_LOGS_DIR, f"run_{ref_str}.log")
        command_str = (
            "python -m syncnet_python.run_syncnet "
            f"--data_dir {DATA_WORK_DIR} --reference {ref_str}"
        )
        logger.debug(f"[run_syncnet] Constructed command: {command_str}")

        process = await asyncio.create_subprocess_shell(
            command_str,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.STDOUT
        )
        stdout_bytes, _ = await process.communicate()
        stdout_decoded = stdout_bytes.decode()
        async with aiofiles.open(log_file, 'w') as f:
            await f.write(stdout_decoded)
        logger.debug(f"[run_syncnet] Written output to log file: {ref_str}")

        if process.returncode != 0:
            error_msg = f"SyncNet failed for reference {ref_str} with return code {process.returncode}"
            logger.error(f"[run_syncnet] {error_msg}")
            raise RuntimeError(error_msg)
        logger.info(f"SyncNet model completed successfully. Log saved to: {ref_str}")
        logger.debug(f"[run_syncnet][EXIT] Returning log_file: {ref_str}")
        return log_file

    @staticmethod
    async def run_pipeline(video_file: str, ref: str) -> None:
        logger.debug(f"[run_pipeline][ENTER] video_file='{video_file}', ref='{ref}'")
        command_str = (
            "python -m syncnet_python.run_pipeline "
            f"--videofile {video_file} --reference {ref}"
        )
        logger.debug(f"[run_pipeline] Constructed command: {command_str}")

        log_file: str = os.path.join(os.path.dirname(FINAL_LOGS_DIR), 'pipeline.log')
        process = await asyncio.create_subprocess_shell(
            command_str,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.STDOUT
        )
        stdout_bytes, _ = await process.communicate()
        stdout_decoded = stdout_bytes.decode()
        
        async with aiofiles.open(log_file, 'w') as f:
            await f.write(stdout_decoded)
        logger.debug(f"[run_pipeline] Written output to log file: {ref}")

        if process.returncode != 0:
            error_msg = f"SyncNet pipeline failed for video {video_file} (ref={ref}) with return code {process.returncode}"
            logger.error(f"[run_pipeline] {error_msg}")
            raise RuntimeError(error_msg)
        logger.info(f"SyncNet pipeline successfully executed for video: {video_file} with reference: {ref}")
        logger.debug(f"[run_pipeline][EXIT] Completed pipeline run for video_file='{video_file}'")

    @staticmethod
    async def prepare_video(input_file: str, original_filename: str) -> Tuple[str, VideoProps, AudioProps, Union[int, float], str, int]:
        logger.debug(f"[prepare_video][ENTER] input_file='{input_file}', original_filename='{original_filename}'")
        ApiUtils.send_websocket_message("Here we go...")
        ApiUtils.send_websocket_message("Setting up our filing system...")

        dir_number_str = await FileUtils.get_next_directory_number(DATA_WORK_PYAVI_DIR)
        reference_number: int = int(dir_number_str)
        logger.debug(f"[prepare_video] Obtained reference_number: {reference_number}")

        ApiUtils.send_websocket_message("Copying your file to work on...")
        temp_copy_path = await FileUtils.copy_file(input_file, original_filename)
        destination_path = os.path.join(DATA_DIR, f"{reference_number}_{original_filename}")
        await FileUtils.move_file(temp_copy_path, destination_path)
        logger.debug(f"[prepare_video] Moved file to destination_path: {destination_path}")
        
        exists = await ApiUtils.run_blocking(os.path.exists, destination_path)
        if not exists:
            error_msg = f"Destination file {destination_path} doesn't exist. Aborting process."
            logger.error(f"[prepare_video] {error_msg}")
            raise RuntimeError(error_msg)
        else:
            logger.debug(f"[prepare_video] Verified destination file exists.")

        vid_props: Optional[VideoProps] = await FFmpegUtils.get_video_properties(input_file)

        logger.debug(f"[prepare_video] Video properties: {vid_props}")
        if vid_props is None:
            error_msg = "Couldn't find any video stream"
            logger.error(f"[prepare_video] {error_msg}")
            raise RuntimeError(error_msg)

        fps: Union[int, float] = vid_props.get('fps')
        ApiUtils.send_websocket_message("Finding out about your file...")

        audio_props: Optional[AudioProps] = await FFmpegUtils.get_audio_properties(input_file)

        logger.debug(f"[prepare_video] Audio properties: {audio_props}")
        if audio_props is None:
            error_msg = "No audio stream found in the video."
            logger.error(f"[prepare_video] {error_msg}")
            raise RuntimeError(error_msg)

        ext: str = os.path.splitext(original_filename)[1].lower()
        if ext == ".avi":
            avi_file: str = destination_path
            logger.debug(f"[prepare_video] File already in AVI format. avi_file set to destination_path: {avi_file}")
        else:
            logger.info("Converting file to avi for processing")
            avi_file = os.path.splitext(destination_path)[0] + "_reencoded.avi"
            logger.debug(f"[prepare_video] Re-encoding to avi. New avi_file: {avi_file}")
            await FFmpegUtils.reencode_to_avi(destination_path, avi_file)

        logger.debug(
            f"[prepare_video][EXIT] Returning avi_file='{avi_file}', vid_props={vid_props}, "
            f"audio_props={audio_props}, fps={fps}, destination_path='{destination_path}', reference_number={reference_number}"
        )
        return avi_file, vid_props, audio_props, fps, destination_path, reference_number

    @staticmethod
    async def perform_sync_iterations(
        corrected_file: str,
        original_filename: str,
        fps: Union[int, float],
        reference_number: int
    ) -> Union[SyncError, Tuple[int, str, int, int]]:
        logger.debug(
            "[DATA][ENTER] perform_sync_iterations -> "
            f"corrected_file='{corrected_file}', original_filename='{original_filename}', "
            f"fps={fps}, reference_number={reference_number}"
        )
        total_shift_ms: int = 0
        iteration_count: int = 0

        for iteration in range(DEFAULT_MAX_ITERATIONS):
            iteration_count = iteration + 1
            iteration_msg: str = f"Pass number {iteration_count} in progress..."
            ApiUtils.send_websocket_message(iteration_msg)
            logger.info(f"[perform_sync_iterations] {iteration_msg}")

            ref_str: str = f"{reference_number:05d}"
            logger.debug(f"[perform_sync_iterations] Using ref_str: {ref_str}")

            await SyncNetUtils.run_pipeline(corrected_file, ref_str)

            ApiUtils.send_websocket_message("Checking how out-of-sync your file was")

            log_file: str = await SyncNetUtils.run_syncnet(ref_str)

            logger.debug(f"[perform_sync_iterations] Obtained log_file: {log_file}")

            ApiUtils.send_websocket_message("Analyzing the results that came back...")
            sync_result = await AnalysisUtils.analyze_syncnet_log(log_file, fps)
            offset_ms: int = sync_result.best_offset_ms 
            
            ApiUtils.send_websocket_message(f"it was {offset_ms}milliseconds out of sync")
            logger.debug(f"[perform_sync_iterations] Computed offset_ms: {offset_ms}")

            if offset_ms == 0:
                if iteration == 0:
                    logger.debug("[perform_sync_iterations] Zero offset on first iteration -> already in sync.")
                    logger.debug(f"[perform_sync_iterations][EXIT] Returning (0, {corrected_file}, {reference_number}, {iteration_count})")
                    return (0, corrected_file, reference_number, iteration_count)
                else:
                    ApiUtils.send_websocket_message("Clip is now perfectly in sync; finishing...")
                    logger.debug(f"[perform_sync_iterations] Ending iterations at iteration_count: {iteration_count}")
                    break

            total_shift_ms += offset_ms
            logger.debug(f"[perform_sync_iterations] Total shift after pass {iteration_count}: {total_shift_ms}")

            offset_msg: str = f"Total shift after pass {iteration_count} will be {total_shift_ms} ms."
            ApiUtils.send_websocket_message(offset_msg)
            logger.info(f"[perform_sync_iterations] {offset_msg}")

            base_name: str = os.path.splitext(original_filename)[0]
            new_corrected_file: str = os.path.join(
                TEMP_PROCESSING_DIR,
                f"corrected_iter{iteration_count}_{base_name}.avi"
            )
            logger.debug(f"[perform_sync_iterations] New corrected file will be: {new_corrected_file}")

            ApiUtils.send_websocket_message("Adjusting the streams in your file...")
            await FFmpegUtils.shift_audio(corrected_file, new_corrected_file, offset_ms)

            exists = await ApiUtils.run_blocking(os.path.exists, new_corrected_file)
            if not exists:
                error_msg = f"Corrected file {new_corrected_file} was not created. Aborting process."
                logger.error(f"[perform_sync_iterations] {error_msg}")
                raise RuntimeError(error_msg)
            corrected_file = new_corrected_file
            reference_number += 1
            logger.debug(f"[perform_sync_iterations] Updated corrected_file: {corrected_file}, updated reference_number: {reference_number}")

        logger.debug(
            f"[DATA][EXIT] perform_sync_iterations -> total_shift_ms={total_shift_ms}, "
            f"corrected_file='{corrected_file}', updated_reference_number={reference_number}, iteration_count={iteration_count}"
        )
        return (total_shift_ms, corrected_file, reference_number, iteration_count)

    @staticmethod
    async def finalize_sync(
        input_file: str,
        original_filename: str,
        total_shift_ms: int,
        reference_number: int,
        fps: Union[int, float],
        destination_path: str,
        vid_props: VideoProps,
        audio_props: AudioProps,
        corrected_file: str
    ) -> Union[str, SyncError]:
        logger.debug(
            "[DATA][ENTER] finalize_sync -> "
            f"input_file='{input_file}', original_filename='{original_filename}', total_shift_ms={total_shift_ms}, "
            f"reference_number={reference_number}, fps={fps}, destination_path='{destination_path}', "
            f"corrected_file='{corrected_file}'"
        )
        final_output_path: str = os.path.join(FINAL_OUTPUT_DIR, f"corrected_{original_filename}")
        logger.debug(f"[finalize_sync] Final output path set to: {final_output_path}")

        ApiUtils.send_websocket_message("Making the final shift...")

        await FFmpegUtils.apply_cumulative_shift(input_file, final_output_path, total_shift_ms)

        logger.debug("[finalize_sync] Applied cumulative shift.")

        ApiUtils.send_websocket_message("Double checking everything...")
        ref_str: str = f"{reference_number:05d}"
        logger.debug(f"[finalize_sync] Using ref_str for final check: {ref_str}")
        await SyncNetUtils.run_pipeline(final_output_path, ref_str)

        final_log: str = os.path.join(FINAL_LOGS_DIR, f"final_output_{ref_str}.log")
        await SyncNetUtils.run_syncnet(ref_str, final_log)

        analysis_result = await AnalysisUtils.analyze_syncnet_log(final_log, fps)
        final_offset: int = analysis_result.best_offset_ms

        logger.debug(f"[finalize_sync] Analyzed final_offset: {final_offset}")

        if final_offset != 0:
            error_msg: str = "final offset incorrect"
            ApiUtils.send_websocket_message(
                "Something went wrong behind the scenes and your clip wasn't synced properly! Please refresh the page and try again."
            )
            logger.error(f"[finalize_sync] {error_msg} -> final_offset={final_offset}")
            return SyncError(
                error=True,
                message="Something went wrong behind the scenes. Your clip wasn't synced properly",
                final_offset=final_offset
            )

        exists = await ApiUtils.run_blocking(os.path.exists, corrected_file)
        if corrected_file != destination_path and exists:
            await FileUtils.cleanup_file(corrected_file)
            logger.debug(f"[finalize_sync] Removed old corrected_file: '{corrected_file}'")

        original_ext: str = os.path.splitext(original_filename)[1].lower()
        if original_ext == ".avi":
            logger.info("[finalize_sync] Original file was AVI -> skipping re-encode.")
            logger.debug(f"[finalize_sync][EXIT] Returning final_output_path: '{final_output_path}'")
            return final_output_path
        else:
            logger.info("[finalize_sync] Re-encoding final output back to original container/codec.")
            original_video_codec: Optional[str] = vid_props.get('codec_name')
            original_audio_codec: Optional[str] = audio_props.get('codec_name')
            restored_final: str = os.path.splitext(final_output_path)[0] + "_restored" + original_ext
            logger.debug(f"[finalize_sync] Restored final path: {restored_final}")
            await FFmpegUtils.reencode_to_original_format(
                final_output_path, restored_final, original_ext,
                original_video_codec, original_audio_codec
            )
            logger.debug(f"[finalize_sync][EXIT] Returning restored_final: '{restored_final}'")
            return restored_final

    @staticmethod
    async def synchronize_video(
        avi_file: str,
        input_file: str,
        original_filename: str,
        vid_props: VideoProps,
        audio_props: AudioProps,
        fps: Union[int, float],
        destination_path: str,
        reference_number: int
    ) -> Union[Tuple[str, bool], SyncError]:
        logger.debug(
            "[DATA][ENTER] synchronize_video -> "
            f"avi_file='{avi_file}', input_file='{input_file}', original_filename='{original_filename}', "
            f"vid_props={vid_props}, audio_props={audio_props}, fps={fps}, "
            f"destination_path='{destination_path}', reference_number={reference_number}"
        )
        ApiUtils.send_websocket_message("Ok, had a look; let's begin to sync...")

        sync_iterations_result = await SyncNetUtils.perform_sync_iterations(
            corrected_file=avi_file,
            original_filename=original_filename,
            fps=fps,
            reference_number=reference_number
        )
        logger.debug(f"[synchronize_video] perform_sync_iterations returned: {sync_iterations_result}")

        if isinstance(sync_iterations_result, SyncError):
            logger.debug(f"[synchronize_video][EXIT] Returning SyncError: {sync_iterations_result}")
            return sync_iterations_result

        total_shift_ms, final_corrected_file, updated_reference_number, iteration_count = sync_iterations_result
        logger.info(
            "[synchronize_video] "
            f"iteration_count={iteration_count}, total_shift_ms={total_shift_ms}, final_corrected_file='{final_corrected_file}', "
            f"updated_reference_number={updated_reference_number}"
        )

        if iteration_count == 1 and total_shift_ms == 0:
            ApiUtils.send_websocket_message(
                "Your clip was already in sync on the first pass; skipping final verification."
            )
            final_output_path: str = os.path.join(FINAL_OUTPUT_DIR, f"corrected_{original_filename}")
            if os.path.splitext(original_filename)[1].lower() == ".avi":
                await FileUtils.copy_file(input_file, final_output_path)
                logger.debug(f"[synchronize_video] Copied original file to final_output_path: {final_output_path}")
            else:
                original_ext: str = os.path.splitext(original_filename)[1].lower()
                original_video_codec: Optional[str] = vid_props.get('codec_name')
                original_audio_codec: Optional[str] = audio_props.get('codec_name')
                restored_final: str = os.path.splitext(final_output_path)[0] + "_restored" + original_ext
                logger.debug(f"[synchronize_video] Re-encoding input file to restored_final: {restored_final}")
                await FFmpegUtils.reencode_to_original_format(
                    input_file, restored_final, original_ext,
                    original_video_codec, original_audio_codec
                )
                final_output_path = restored_final

            logger.debug(f"[synchronize_video][EXIT] Returning (final_output_path='{final_output_path}', already_in_sync=True)")
            return (final_output_path, True)

        final_output_path = await SyncNetUtils.finalize_sync(
            input_file=input_file,
            original_filename=original_filename,
            total_shift_ms=total_shift_ms,
            reference_number=updated_reference_number,
            fps=fps,
            destination_path=destination_path,
            vid_props=vid_props,
            audio_props=audio_props,
            corrected_file=final_corrected_file
        )

        if isinstance(final_output_path, SyncError):
            logger.debug(f"[synchronize_video][EXIT] Returning SyncError from finalize_sync: {final_output_path}")
            return final_output_path

        logger.debug(f"[synchronize_video][EXIT] Returning (final_output_path='{final_output_path}', already_in_sync=False)")
        return (final_output_path, False)

    @staticmethod
    async def verify_synchronization(final_path: str, ref_str: str, fps: Union[int, float]) -> None:
        logger.debug(
            f"[verify_synchronization][ENTER] final_path='{final_path}', ref_str='{ref_str}', fps={fps}"
        )
        logger.info("[verify_synchronization] Starting final verification pipeline...")
        await SyncNetUtils.run_pipeline(final_path, ref_str)

        final_log: str = os.path.join(FINAL_LOGS_DIR, f"final_output_{ref_str}.log")
        await SyncNetUtils.run_syncnet(ref_str, final_log)

        final_offset: int = await AnalysisUtils.analyze_syncnet_log(final_log, fps)
        logger.info(f"[verify_synchronization] final_offset -> {final_offset} ms")
        logger.debug("[verify_synchronization][EXIT]")
"""
A custom logging handler that sends log messages over WebSocket.
"""

import asyncio
import logging
from api.connection_manager import broadcast


class WebSocketLogHandler(logging.Handler):
    def emit(self, record: logging.LogRecord) -> None:
        """
        Emits a log record over a WebSocket.
        
        Args:
            record (logging.LogRecord): The log record to send.
        """
        msg: str = self.format(record)
        asyncio.create_task(broadcast(msg))
