import os
import logging
from api.utils.file_utils import FileUtils
from api.utils.ffmpeg_utils import FFmpegUtils
from api.utils.syncnet_utils import SyncNetUtils
from api.utils.analysis_utils import AnalysisUtils
from api.config.settings import (
    RUN_LOGS_DIR, LOGS_DIR, FINAL_LOGS_DIR, DEFAULT_MAX_ITERATIONS,
    DEFAULT_TOLERANCE_MS, TEMP_PROCESSING_DIR, DATA_WORK_PYAVI_DIR,
    FINAL_OUTPUT_DIR
)

logger = logging.getLogger('process_video')

def process_video(input_file, original_filename):
    """
    synchronizes a video file by processing it with the offset generated by the syncnet pipeline.
      - prepares necessary directories
      - copies the input file to a temporary folder
      - moves the file to the pipeline's data directory
      - retrieves the video's fps
      - performs an initial sync step; if the offset is 0, ends processing immediately
      - otherwise, runs further iterations to correct the offset
      - applies a cumulative shift to generate the final synchronized video and returns its final path

    returns:
        str or none: path to the final synchronized video, or none if processing failed.
    """
    try:
        logger.info("starting video synchronization process...")

        # ensure that all required directories exist
        directories = [
            LOGS_DIR, RUN_LOGS_DIR, FINAL_LOGS_DIR, TEMP_PROCESSING_DIR,
            DATA_WORK_PYAVI_DIR, FINAL_OUTPUT_DIR
        ]
        FileUtils.prepare_directories(directories)

        # get the next directory reference number as an integer (e.g. 00014)
        reference_number = int(FileUtils.get_next_directory_number())

        # copy the input file to the temporary folder and move it into the data work directory
        temp_copy_path = FileUtils.copy_input_to_temp(input_file, original_filename)
        destination_path = FileUtils.move_to_data_work(temp_copy_path, reference_number)

        # if the destination file wasn't created, abort processing
        if not os.path.exists(destination_path):
            logger.error(f"destination file {destination_path} does not exist. aborting process.")
            return None

        # retrieve the video's frames per second (fps) using ffprobe
        fps = FFmpegUtils.get_video_fps(input_file)
        if fps is None:
            logger.error("could not retrieve fps from the video. aborting process.")
            return None

        total_shift_ms = 0  # initialize cumulative shift (in milliseconds)
        corrected_file = destination_path  # start processing with the file in the data work directory

        # --- perform initial sync step (iteration 0) ---
        ref_str = f"{reference_number:05d}"  # format reference string with leading zeros
        # run the pipeline and syncnet model on the current file
        SyncNetUtils.run_pipeline(corrected_file, ref_str)
        log_file = SyncNetUtils.run_syncnet(ref_str)
        # analyze the log to determine the offset (in milliseconds)
        offset_ms = AnalysisUtils.analyze_syncnet_log(log_file, fps)
        total_shift_ms += offset_ms
        logger.info(f"total shift after initial iteration: {total_shift_ms} ms")

        # if the first iteration's offset is 0, consider the file already synchronized
        if offset_ms == 0:
            logger.info("first run offset is 0 ms; file is already synchronized, ending processing.")
            # define final output path
            final_output_path = os.path.join(FINAL_OUTPUT_DIR, f"corrected_{original_filename}")
            # apply a 0-ms cumulative shift (this effectively copies the input file to the final output)
            FFmpegUtils.apply_cumulative_shift(input_file, final_output_path, 0)
            # return immediately without further processing
            return final_output_path

        # --- perform subsequent sync iterations (if needed) ---
        for iteration in range(1, DEFAULT_MAX_ITERATIONS):
            reference_number += 1  # increment reference number for this iteration
            ref_str = f"{reference_number:05d}"
            # run the pipeline and syncnet model on the current corrected file
            SyncNetUtils.run_pipeline(corrected_file, ref_str)
            log_file = SyncNetUtils.run_syncnet(ref_str)
            offset_ms = AnalysisUtils.analyze_syncnet_log(log_file, fps)
            total_shift_ms += offset_ms
            logger.info(f"total shift after iteration {iteration + 1}: {total_shift_ms} ms")

            # if the current offset is within tolerance, exit the loop
            if abs(offset_ms) <= DEFAULT_TOLERANCE_MS:
                logger.info("synchronization offset is within the acceptable tolerance.")
                break

            # apply the offset shift to generate a new corrected file for the next iteration
            new_corrected_file = os.path.join(
                TEMP_PROCESSING_DIR,
                f"corrected_iter{iteration + 1}_{original_filename}"
            )
            FFmpegUtils.shift_audio(corrected_file, new_corrected_file, offset_ms)
            # if the new corrected file was not created, abort processing
            if not os.path.exists(new_corrected_file):
                logger.error(f"corrected file {new_corrected_file} was not created, ending process.")
                return None

            corrected_file = new_corrected_file  # update for next iteration

        # apply the cumulative shift to generate the final output file
        final_output_path = os.path.join(FINAL_OUTPUT_DIR, f"corrected_{original_filename}")
        FFmpegUtils.apply_cumulative_shift(input_file, final_output_path, total_shift_ms)

        # run final verification only if a shift was applied (i.e. total_shift_ms != 0)
        if total_shift_ms != 0:
            AnalysisUtils.verify_synchronization(
                final_output_path, f"{reference_number:05d}", fps, DEFAULT_TOLERANCE_MS
            )
        else:
            logger.info("total shift is 0, skipping final verification.")

        # clean up the temporary corrected file if it is not the original destination
        if corrected_file != destination_path and os.path.exists(corrected_file):
            os.remove(corrected_file)

        # return the final processed video file path
        return final_output_path

    except Exception as e:
        logger.error(f"an error occurred during video processing: {e}")
        return None
